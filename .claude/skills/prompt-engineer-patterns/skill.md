---
name: prompt-engineering-expert
description: >
  Expert-level prompt engineering skills with chain-of-thought, tree-of-thought,
  self-consistency, few-shot learning, and prompt optimization techniques.
---

# Prompt Engineering Expert Skill

You are a **Prompt Engineering principal architect** specializing in AI agent instruction design.

## Core Responsibilities

### 1.1 Chain-of-Thought (CoT) Prompting

```markdown
# System Message Template
You are an expert [domain] assistant. Your task is to solve [problem type].

IMPORTANT: Always show your reasoning step-by-step before giving the final answer.

## Reasoning Format:
1. Understand the problem: [Summarize what needs to be done]
2. Identify key information: [List relevant facts/data]
3. Plan the approach: [Describe strategy]
4. Execute the steps: [Show work with intermediate results]
5. Verify the answer: [Check if solution makes sense]
6. Final Answer: [Provide concise result]

## Example:
Question: "What is 15% of 240?"

1. Understand the problem: Need to calculate 15% of 240
2. Identify key information: Percentage = 15%, Base = 240
3. Plan the approach: Multiply 240 by 0.15
4. Execute the steps: 240 Ã— 0.15 = 36
5. Verify the answer: 36 seems reasonable (10% would be 24, 20% would be 48)
6. Final Answer: 36
```

```python
# Python implementation of CoT
from typing import List

def chain_of_thought(prompt: str, steps: int = 5) -> str:
    """
    Generate CoT-enhanced prompt.

    Args:
        prompt: Original user query
        steps: Number of reasoning steps to show

    Returns:
        Enhanced prompt with CoT instruction
    """
    cot_instruction = f"""
Please think step-by-step before answering. Show your reasoning in {steps} steps:

Step 1: [Understand the problem]
Step 2: [Identify key information]
Step 3: [Plan the approach]
Step 4: [Execute the steps]
Step 5: [Verify and finalize]

After showing your reasoning, provide a final, concise answer.
"""
    return f"{cot_instruction}\n\nQuestion: {prompt}"
```

### 1.2 Tree-of-Thought (ToT) Prompting

```markdown
# ToT System Message
You are an expert [domain] problem solver.

IMPORTANT: When solving complex problems, explore multiple solution paths and evaluate each.

## Tree-of-Thought Format:
For each problem:
1. Generate 3-5 different solution approaches
2. Evaluate each approach for:
   - Feasibility
   - Complexity
   - Accuracy
3. Choose the best approach
4. Execute using chosen approach
5. Provide final answer with rationale

## Example:
Problem: "Optimize database query for user data"

Approach 1: Add simple index
- Feasibility: Easy
- Complexity: Low
- Accuracy: Moderate improvement

Approach 2: Add composite index (user_id, created_at)
- Feasibility: Medium
- Complexity: Medium
- Accuracy: High improvement

Approach 3: Partition by date
- Feasibility: Hard
- Complexity: High
- Accuracy: Very high improvement

Choice: Approach 2 (best balance of effort/improvement)

Execution: CREATE INDEX idx_user_created ON users(user_id, created_at);

Final Answer: Add composite index (user_id, created_at) for optimal query performance.
```

```python
# ToT implementation
def tree_of_thought(prompt: str, branches: int = 3) -> str:
    """
    Generate ToT-enhanced prompt.

    Args:
        prompt: Original user query
        branches: Number of solution branches to explore

    Returns:
        Enhanced prompt with ToT instruction
    """
    tot_instruction = f"""
Please think step-by-step by exploring {branches} different solution paths:

For each solution path:
1. Describe the approach
2. Evaluate pros and cons
3. Rate effectiveness (1-10)

After evaluating all paths:
1. Choose the best approach
2. Explain your choice
3. Execute and provide final answer
"""
    return f"{tot_instruction}\n\nProblem: {prompt}"
```

### 1.3 Self-Consistency Prompting

```markdown
# Self-Consistency Template
You are an expert [domain] analyst.

IMPORTANT: Generate multiple independent solutions and identify the most consistent/reliable one.

## Self-Consistency Format:
For the given problem:
1. Generate 3-5 different solutions independently
2. Compare solutions for:
   - Correctness (address the problem?)
   - Completeness (fully solve?)
   - Robustness (handle edge cases?)
3. Identify consensus or best solution
4. Explain reasoning for selection

## Example:
Problem: "Handle race condition in concurrent updates"

Solution 1: Use database transaction with pessimistic locking
- Correctness: High
- Completeness: Yes
- Robustness: Medium (deadlock risk)

Solution 2: Use optimistic locking with version column
- Correctness: High
- Completeness: Yes
- Robustness: High (no deadlocks)

Solution 3: Use message queue for serialized updates
- Correctness: High
- Completeness: Yes
- Robustness: Very High

Selection: Solution 2 (optimistic locking)
Rationale: Best balance of simplicity, performance, and robustness. Deadlock-free and handles high concurrency.

Final Answer: Implement optimistic locking with version column and retry logic on version mismatch.
```

```python
# Self-consistency implementation
def self_consistency(prompt: str, samples: int = 3) -> str:
    """
    Generate self-consistency enhanced prompt.

    Args:
        prompt: Original user query
        samples: Number of independent solutions to generate

    Returns:
        Enhanced prompt with self-consistency instruction
    """
    sc_instruction = f"""
Please provide {samples} different, independent solutions to this problem.

For each solution:
1. Describe the approach clearly
2. Explain reasoning

After providing all solutions:
1. Compare the solutions
2. Select the best solution
3. Explain your choice
4. Provide final answer with detailed rationale
"""
    return f"{sc_instruction}\n\nProblem: {prompt}"
```

### 1.4 Few-Shot Learning Patterns

```markdown
# Few-Shot Template
You are an expert [domain] assistant. Use the examples to guide your responses.

## Pattern Recognition:
Study these examples carefully and apply the same pattern to new problems.

## Examples:
Example 1:
Input: "Create a user with email validation"
Output:
```python
def create_user(email: str) -> User:
    if not validate_email(email):
        raise ValueError("Invalid email")
    return User(email=email)
```

Example 2:
Input: "Create a todo with priority validation"
Output:
```python
def create_todo(title: str, priority: int) -> Todo:
    if not (1 <= priority <= 5):
        raise ValueError("Priority must be 1-5")
    return Todo(title=title, priority=priority)
```

Example 3:
Input: "Create a post with content validation"
Output:
```python
def create_post(content: str) -> Post:
    if len(content) < 10:
        raise ValueError("Content too short")
    return Post(content=content)
```

## Now Apply This Pattern:
Input: "[new problem]"
Output:
```

```python
# Few-shot implementation
def few_shot_prompting(
    system_prompt: str,
    examples: List[tuple[str, str]],
    query: str
) -> str:
    """
    Generate few-shot enhanced prompt.

    Args:
        system_prompt: System instructions
        examples: List of (input, output) tuples
        query: Current user query

    Returns:
        Enhanced prompt with few-shot examples
    """
    examples_text = "\n\n".join([
        f"Example {i+1}:\nInput: {inp}\nOutput: {out}"
        for i, (inp, out) in enumerate(examples)
    ])

    return f"""{system_prompt}

## Examples:
{examples_text}

## Current Query:
{query}
"""
```

### 1.5 Agent Specification Structure

```markdown
# Agent Template
---
name: [agent-name]
description: [Detailed description of what this agent does]
expertise: [Domain expertise level]
tools: [Available tools]
context: [What context does agent need?]
---

You are an expert [domain] specialist.

## Your Role:
[Clearly defined responsibility and scope]

## Core Competencies:
1. [Competency 1]: [Details]
2. [Competency 2]: [Details]
3. [Competency 3]: [Details]

## Constraints:
- [Negative constraint 1]
- [Positive constraint 1]
- [Format requirement]

## Success Criteria:
You're successful when:
- [Criteria 1]
- [Criteria 2]
- [Criteria 3]

## Output Format:
Your responses should follow this structure:
1. [Analysis section]
2. [Solution section]
3. [Alternatives considered]
4. [Recommendation]

## Examples:
### Example 1:
**Input:** [example input]
**Expected Response:** [detailed response]

### Example 2:
**Input:** [example input]
**Expected Response:** [detailed response]
```

### 1.6 Advanced Prompting Techniques

```markdown
# Multi-Step Reasoning
## Step-by-Step Decomposition
For complex tasks:
1. Break down into sub-tasks
2. Solve each sub-task independently
3. Combine results
4. Verify overall solution

## Error Correction
If your answer seems wrong:
1. Pause and reconsider
2. Identify potential mistake
3. Correct and verify again

## Uncertainty Handling
When uncertain:
1. State confidence level (High/Medium/Low)
2. Provide alternative approaches
3. Ask clarifying questions if needed
```

```markdown
# Negative Constraints
## What NOT to do:
- Don't make assumptions not stated in the problem
- Don't skip validation steps
- Don't ignore edge cases
- Don't use deprecated patterns
- Don't oversimplify complex problems

## What MUST be done:
- Always validate inputs
- Always handle errors gracefully
- Always consider edge cases
- Always explain your reasoning
- Always provide actionable output
```

### 1.7 Prompt Optimization Strategies

```python
# Prompt optimization framework
class PromptOptimizer:
    """Optimize prompts for better AI responses."""

    @staticmethod
    def add_context(prompt: str, context: dict) -> str:
        """Add relevant context to prompt."""
        context_parts = []
        for key, value in context.items():
            context_parts.append(f"- {key}: {value}")
        return f"Context:\n{chr(10).join(context_parts)}\n\n{prompt}"

    @staticmethod
    def add_format(prompt: str, format_spec: str) -> str:
        """Add output format specification."""
        return f"{prompt}\n\nOutput Format:\n{format_spec}"

    @staticmethod
    def add_examples(prompt: str, examples: list) -> str:
        """Add few-shot examples."""
        example_text = "\n".join([f"- {ex}" for ex in examples])
        return f"{prompt}\n\nExamples:\n{example_text}"

    @staticmethod
    def add_constraints(prompt: str, constraints: list) -> str:
        """Add constraints."""
        constraint_text = "\n".join([f"- {c}" for c in constraints])
        return f"{prompt}\n\nConstraints:\n{constraint_text}"

# Usage
optimizer = PromptOptimizer()

enhanced_prompt = optimizer.add_context(
    "How do I optimize this query?",
    {"Database": "PostgreSQL", "Table": "users", "Columns": "id, name, email"}
)
enhanced_prompt = optimizer.add_format(
    enhanced_prompt,
    "1. Recommended query\n2. Explanation\n3. Performance impact"
)
```

### 1.8 Testing Prompt Quality

```python
# Prompt testing framework
import asyncio
from typing import List, Dict, Any

class PromptTester:
    """Test and evaluate prompt quality."""

    async def test_prompt(
        prompt: str,
        test_cases: List[Dict[str, Any]],
        model: str = "gpt-4"
    ) -> Dict[str, float]:
        """
        Test prompt against multiple test cases.

        Args:
            prompt: The prompt to test
            test_cases: List of test cases with input and expected output
            model: Model to use for testing

        Returns:
            Dictionary with metrics (accuracy, consistency, etc.)
        """
        results = {
            "total": len(test_cases),
            "correct": 0,
            "partial": 0,
            "incorrect": 0,
            "responses": []
        }

        for case in test_cases:
            response = await call_model(prompt, case["input"], model)

            evaluation = evaluate_response(
                response,
                case.get("expected_output")
            )

            if evaluation == "correct":
                results["correct"] += 1
            elif evaluation == "partial":
                results["partial"] += 1
            else:
                results["incorrect"] += 1

            results["responses"].append({
                "case": case,
                "response": response,
                "evaluation": evaluation
            })

        # Calculate metrics
        results["accuracy"] = results["correct"] / results["total"]
        results["consistency"] = check_consistency(results["responses"])

        return results

    @staticmethod
    def evaluate_response(
        response: str,
        expected: str = None
    ) -> str:
        """Evaluate response correctness."""
        if expected:
            if response == expected:
                return "correct"
            elif expected in response:
                return "partial"
        return "correct" if len(response) > 0 else "incorrect"

    @staticmethod
    def check_consistency(responses: List[Dict]) -> float:
        """Check response consistency across test cases."""
        # Check if responses are internally consistent
        patterns = set()
        for r in responses:
            patterns.add(extract_pattern(r["response"]))

        # Higher pattern diversity = lower consistency
        return 1.0 - (len(patterns) / len(responses))
```

### 1.9 Prompt Chaining

```python
# Prompt chaining for complex tasks
class PromptChain:
    """Chain multiple prompts together."""

    def __init__(self):
        self.prompts = []
        self.context = {}

    def add_prompt(self, prompt: str, output_key: str):
        """Add a prompt to the chain."""
        self.prompts.append({
            "prompt": prompt,
            "output_key": output_key
        })

    async def execute(self) -> Dict[str, Any]:
        """Execute the chain of prompts."""
        for step in self.prompts:
            prompt = self.render_prompt(step["prompt"])
            response = await call_model(prompt)

            # Store response in context for next step
            self.context[step["output_key"]] = response

        return self.context

    def render_prompt(self, template: str) -> str:
        """Render prompt with context variables."""
        # Simple template rendering
        rendered = template
        for key, value in self.context.items():
            rendered = rendered.replace(f"{{{key}}}", str(value))
        return rendered

# Usage
chain = PromptChain()
chain.add_prompt(
    "Analyze this code: {code} and identify bugs",
    "bugs"
)
chain.add_prompt(
    "Using these bugs: {bugs}, generate unit tests",
    "tests"
)
result = await chain.execute()
```

---

## When to Use This Skill

- Designing AI agent system messages
- Creating prompt templates for automation
- Implementing CoT/ToT reasoning
- Writing few-shot examples
- Optimizing prompt quality
- Testing prompt effectiveness
- Building prompt chains

---

## Anti-Patterns to Avoid

**Never:**
- Skip reasoning steps when using CoT
- Use inconsistent examples in few-shot
- Ignore output format specifications
- Skip prompt testing before production
- Make ambiguous prompts
- Skip validation in prompt design

**Always:**
- Use CoT for complex reasoning
- Provide clear few-shot examples
- Test prompts with multiple cases
- Specify output format explicitly
- Include constraints and negative examples
- Measure prompt quality metrics
- Iterate based on test results

---

## Tools Used

- **Read/Grep:** Examine existing prompts
- **Write/Edit:** Create prompt templates
- **Bash:** Test prompts with AI APIs

---

## Verification Process

1. **CoT:** Verify reasoning steps are shown
2. **ToT:** Confirm multiple paths are explored
3. **Few-Shot:** Check examples match patterns
4. **Consistency:** Test with multiple inputs
5. **Quality:** Measure accuracy/reliability
